{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DP_FP.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCMFppP2T9Wc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from skimage.io import imread\n",
        "import random\n",
        "import seaborn as sns\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import InputLayer,Conv2D,MaxPool2D,Flatten,Dropout,Dense, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l1_l2,l1,l2\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
        "from keras import models\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import f_classif"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KZmaOU6bUC1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/DeepLearning/"
      ],
      "metadata": {
        "id": "qlzy5ARLva3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "mATPv5E4vtNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infile=open('partially_covered.pkl', 'rb')\n",
        "X_partially_covered=pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile=open('not_face.pkl', 'rb')\n",
        "X_not_face=pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile=open('not_covered.pkl', 'rb')\n",
        "X_not_covered=pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile=open('fully_covered.pkl', 'rb')\n",
        "X_fully_covered=pickle.load(infile)\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "Yz2FKjMBvuBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_partially_covered=np.array(X_partially_covered)\n",
        "X_not_face=np.array(X_not_face)\n",
        "X_not_covered=np.array(X_not_covered)\n",
        "X_fully_covered=np.array(X_fully_covered)\n",
        "\n",
        "print(f\"Partially covered shape: {X_partially_covered.shape}\")\n",
        "print(f\"Not face shape: {X_not_face.shape}\")\n",
        "print(f\"Not covered shape: {X_not_covered.shape}\")\n",
        "print(f\"Fully covered shape: {X_fully_covered.shape}\")"
      ],
      "metadata": {
        "id": "O9QMXng_wHQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.concatenate((X_not_covered,X_fully_covered),axis=0)\n",
        "Y=np.concatenate((np.repeat(0, X_not_covered.shape[0]),np.repeat(1, X_fully_covered.shape[0])),axis=0).reshape(-1,1)\n",
        "one = OneHotEncoder()\n",
        "Y=one.fit_transform(Y).toarray()"
      ],
      "metadata": {
        "id": "4NXYKWoXwRPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.1,random_state=0)\n",
        "X_train,X_val,Y_train,Y_val = train_test_split(X_train,Y_train,test_size=0.15/(1-0.1),random_state=1)\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "Ld1yM9XywT0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints = ModelCheckpoint(\n",
        "                            \"checkpoint_mejor_modelo_v2.h5\",\n",
        "                            monitor='val_loss', ## que funcion monitorea para crear el checkpoint\n",
        "                            verbose=10, ## imprima que guardo un checkpoint\n",
        "                            save_best_only=True, ## solo guarde el mejor model\n",
        "                            save_weights_only=False, ## Nos guarda la arquitectura y los pesos\n",
        "                            mode=\"min\", ### Cuando guardar el checkpoint, en este caso, cada nuevo minimo en la funcion de val_loss\n",
        "                            save_freq='epoch') ### Cada cuando guarda el checkpoint\n"
      ],
      "metadata": {
        "id": "3kHBZ3h7wWnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(64,64,1))) ## las dimensiones de la imagen\n",
        "model.add(Conv2D(filters= 16 , kernel_size=(2,2),strides=(1,1), padding=\"same\", activation = \"relu\" ))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid')) \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters= 32 , kernel_size=(3,3),strides=(1,1), padding=\"same\", activation = \"sigmoid\" ))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid'))  \n",
        "model.add(Conv2D(filters= 64 , kernel_size=(2,2),strides=(1,1), padding=\"same\", activation = \"relu\" ))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid')) \n",
        "model.add(Flatten()) ## convetimos un tensor de cualesquiera dimensiones a un vector \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=256, activation =\"relu\"))\n",
        "model.add(Dense(units=128, activation =\"sigmoid\"))\n",
        "model.add(Dense(units=64, activation =\"relu\", kernel_regularizer= l1_l2(0.01,0.01)))\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=BinaryCrossentropy(), optimizer=Adam(0.0001,), metrics= [\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "coaodAjPHE1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,Y_train, epochs= 100, batch_size =64, validation_data=(X_val,Y_val),callbacks=[checkpoints])\n"
      ],
      "metadata": {
        "id": "FZm1ptBXHJEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(model.history.history['loss'], label='Training', linewidth=2)\n",
        "plt.plot(model.history.history['val_loss'], label='Validation', linewidth=2)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(model.history.history['accuracy'], label='Training', linewidth=2)\n",
        "plt.plot(model.history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x0AUyNTPHxFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model(\"checkpoint_mejor_modelo_v2.h5\")"
      ],
      "metadata": {
        "id": "EDbH1UUMJOxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = best_model.evaluate(x=X_test, y=Y_test)"
      ],
      "metadata": {
        "id": "EQR2-j2iJaup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_hat = best_model.predict(x=X_test)\n",
        "\n",
        "Y_test_hat_cat = np.argmax(Y_test_hat, axis=1)\n",
        "Y_test_cat = np.argmax(Y_test, axis=1)\n",
        "cm=confusion_matrix(y_true=Y_test_cat, y_pred=Y_test_hat_cat)\n",
        "\n",
        "print(cm)\n",
        "print(\"\\n\")\n",
        "print(np.round(np.transpose(np.transpose(cm)/cm.sum(axis=1)),3)*100)"
      ],
      "metadata": {
        "id": "OyM-_bdXJdK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "metadata": {
        "id": "10Tt_K7LJhWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm/np.sum(cm), annot=True, \n",
        "            fmt='.2%', cmap='Blues')"
      ],
      "metadata": {
        "id": "PAxHWobgJ2Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(X_test)\n",
        "\n",
        "total = 0\n",
        "accurate = 0\n",
        "accurateindex = []\n",
        "wrongindex = []\n",
        "\n",
        "for i in range(len(ypred)):\n",
        "    if np.argmax(ypred[i]) == np.argmax(Y_test[i]):\n",
        "        accurate += 1\n",
        "        accurateindex.append(i)\n",
        "    else:\n",
        "        wrongindex.append(i)\n",
        "        \n",
        "    total += 1\n",
        "    \n",
        "print('Total-test-data;', total, '\\taccurately-predicted-data:', accurate, '\\t wrongly-predicted-data: ', total - accurate)\n",
        "print('Accuracy:', round(accurate/total*100, 3), '%')"
      ],
      "metadata": {
        "id": "Nq2G2LGHJ_OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label=['Not covered', 'Fully covered']\n",
        "imidx = random.sample(wrongindex, k=9)# replace with 'wrongindex'\n",
        "\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=(15, 12))\n",
        "\n",
        "n = 0\n",
        "for row in range(nrows):\n",
        "    for col in range(ncols):\n",
        "            ax[row,col].imshow(X_test[imidx[n]], cmap='gray')\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(label[np.argmax(ypred[imidx[n]])], label[np.argmax(Y_test[imidx[n]])]))\n",
        "            n += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "alLFpozTM2fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label=['Not covered', 'Fully covered']\n",
        "imidx = random.sample(accurateindex, k=9)# replace with 'wrongindex'\n",
        "\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=(15, 12))\n",
        "\n",
        "n = 0\n",
        "for row in range(nrows):\n",
        "    for col in range(ncols):\n",
        "            ax[row,col].imshow(X_test[imidx[n]], cmap='gray')\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(label[np.argmax(ypred[imidx[n]])], label[np.argmax(Y_test[imidx[n]])]))\n",
        "            n += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Baq4KTKNFi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred = model.predict(X_test)\n",
        "\n",
        "Ypred = np.argmax(Ypred, axis=1)\n",
        "Ytrue = np.argmax(Y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(Ytrue, Ypred)\n",
        "plt.figure(figsize=(12, 12))\n",
        "ax = sns.heatmap(cm, cmap=\"rocket_r\", fmt=\".01f\",annot_kws={'size':16}, annot=True, square=True, xticklabels=label, yticklabels=label)\n",
        "ax.set_ylabel('Actual', fontsize=20)\n",
        "ax.set_xlabel('Predicted', fontsize=20)"
      ],
      "metadata": {
        "id": "r8st7i7uNZMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "1po43c7qQJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Ytrue, Ypred))"
      ],
      "metadata": {
        "id": "D57KEXeUVSIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_EtIj9dz-cpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}